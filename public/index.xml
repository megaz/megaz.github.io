<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>Zahir Abdi</title>
    <description></description>
    <link>https://megaz.github.io/</link>
    
    <language>en</language>
    <copyright>Copyright 2025, Calvin Tran</copyright>
    <lastBuildDate>Fri, 19 Sep 2025 00:00:00 +0000</lastBuildDate>
    <generator>Hugo - gohugo.io</generator>
    <docs>http://cyber.harvard.edu/rss/rss.html</docs>
    <atom:link href="https://megaz.github.io//atom.xml" rel="self" type="application/atom+xml"/>
    
    
    <item>
      <title>The Migration That Cut Our UN Data Pipeline Time by 87%</title>
      <link>https://megaz.github.io/posts/pandas-to-polars-migration/</link>
      <description>&lt;p&gt;&lt;em&gt;How we achieved up to 29x speedups and 50% memory reduction in our UN data processing pipeline.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At &lt;a href=&#34;https://unstats.un.org/UNSDWebsite/undatacommons/&#34;&gt;UN Data Commons&lt;/a&gt;, we work with billion-row statistical datasets spanning 200+ countries, 70+ years, and multiple demographic and economic indicators. Our Pandas-based pipeline served us well for smaller datasets but as volumes grew, performance and reliability broke down.&lt;/p&gt;
&lt;p&gt;Migrating to &lt;a href=&#34;https://pola.rs&#34;&gt;Polars&lt;/a&gt; was not just a library switch. It was a fundamental rethinking of how we process data at scale. The result: order-of-magnitude performance gains, 50â€“75% lower memory usage, and zero out-of-memory crashes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case in point (ILO dataset ETL):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pandas:&lt;/strong&gt; 45 minutes, 8GB RAM, occasional crashes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Polars:&lt;/strong&gt; 6 minutes, 2GB RAM, stable every time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The performance bottlenecks that forced our migration&lt;/li&gt;
&lt;li&gt;Technical evaluation of modern data processing alternatives&lt;/li&gt;
&lt;li&gt;Why Polars emerged as the clear winner&lt;/li&gt;
&lt;li&gt;Migration strategy and implementation details&lt;/li&gt;
&lt;li&gt;Real-world performance results and impact&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;performance-results-at-a-glance&#34;&gt;Performance Results at a Glance&lt;/h2&gt;
&lt;div class=&#34;performance-metrics&#34;&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Metric&lt;/th&gt;
          &lt;th&gt;Pandas&lt;/th&gt;
          &lt;th&gt;Polars&lt;/th&gt;
          &lt;th&gt;Improvement&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Processing Time&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;45 minutes&lt;/td&gt;
          &lt;td&gt;6 minutes&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;7.5x faster&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;8GB RAM&lt;/td&gt;
          &lt;td&gt;2GB RAM&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;75% reduction&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Memory Errors&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Frequent&lt;/td&gt;
          &lt;td&gt;Zero&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;100% reliability&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img
  src=&#34;https://megaz.github.io/images/processing_time_vs_size_linear.png&#34;
  alt=&#34;Processing Time vs Dataset Size&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;


&lt;em&gt;Processing time comparison showing Polars&amp;rsquo; consistent performance across different dataset sizes&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;https://megaz.github.io/images/memory_vs_size_pretty.png&#34;
  alt=&#34;Memory Usage Comparison&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;


&lt;em&gt;Memory usage comparison demonstrating Polars&amp;rsquo; efficient memory management&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-challenge-scaling-un-data-processing&#34;&gt;The Challenge: Scaling UN Data Processing&lt;/h2&gt;
&lt;p&gt;Our UN Data Commons platform processes massive statistical datasets for policy analysis and users across the world, handling complex multi-dimensional data across:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Population data&lt;/strong&gt;: 200+ countries, 70+ years, multiple demographic indicators&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Economic indicators&lt;/strong&gt;: GDP, trade flows, development metrics, financial flows&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Social data&lt;/strong&gt;: Education, health, gender equality, human development statistics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environmental data&lt;/strong&gt;: Climate indicators, resource usage, sustainability metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our datasets range from millions to hundreds of millions of rows, with complex hierarchical structures and varying data quality across different sources. While our Pandas-based pipeline was working well for smaller datasets, we began hitting scalability challenges as data volumes grew:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Processing times increasing from minutes to hours for complex aggregations&lt;/li&gt;
&lt;li&gt;Memory consumption approaching resource limits (8-16GB datasets on 32GB instances)&lt;/li&gt;
&lt;li&gt;Occasional out-of-memory errors during large joins&lt;/li&gt;
&lt;li&gt;Performance bottlenecks in specific operations like &lt;code&gt;iterrows()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-problem-pandas-at-scale&#34;&gt;The Problem: Pandas at Scale&lt;/h2&gt;
&lt;p&gt;Our UN Data Commons project processes massive statistical datasets for policy analysis, handling complex multi-dimensional data across 200+ countries, 70+ years, and multiple demographic indicators. While Pandas worked well for smaller datasets, we hit scalability challenges as data volumes grew:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Processing times increasing from minutes to hours for complex aggregations&lt;/li&gt;
&lt;li&gt;Memory consumption approaching resource limits (8-16GB datasets on 32GB instances)&lt;/li&gt;
&lt;li&gt;Occasional out-of-memory errors during large joins&lt;/li&gt;
&lt;li&gt;Performance bottlenecks in specific operations like &lt;code&gt;iterrows()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One particular function using &lt;code&gt;iterrows()&lt;/code&gt; was taking over 2 hours to process 100,000 rows from our ILO dataset - a clear optimization target.&lt;/p&gt;
&lt;h2 id=&#34;technical-evaluation-why-polars-won&#34;&gt;Technical Evaluation: Why Polars Won&lt;/h2&gt;
&lt;p&gt;We evaluated five modern data processing frameworks against our criteria: performance, scalability, ecosystem compatibility, and development experience.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Framework&lt;/th&gt;
          &lt;th&gt;Performance vs Pandas&lt;/th&gt;
          &lt;th&gt;Memory Usage&lt;/th&gt;
          &lt;th&gt;Learning Curve&lt;/th&gt;
          &lt;th&gt;Production Ready&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Dask&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;2-3x faster&lt;/td&gt;
          &lt;td&gt;Similar&lt;/td&gt;
          &lt;td&gt;Low&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Modin&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;1-2x faster&lt;/td&gt;
          &lt;td&gt;Similar&lt;/td&gt;
          &lt;td&gt;Low&lt;/td&gt;
          &lt;td&gt;No&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Vaex&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;3-5x faster&lt;/td&gt;
          &lt;td&gt;50% less&lt;/td&gt;
          &lt;td&gt;High&lt;/td&gt;
          &lt;td&gt;No&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;2-4x faster&lt;/td&gt;
          &lt;td&gt;30% less&lt;/td&gt;
          &lt;td&gt;High&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Polars&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;8-15x faster&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;50% less&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Medium&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our benchmarking revealed that Polars consistently outperformed all alternatives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Small datasets (&amp;lt; 1M rows)&lt;/strong&gt;: Polars 3-5x faster than Pandas, 2x faster than Dask&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medium datasets (1-10M rows)&lt;/strong&gt;: Polars 5-8x faster than Pandas, 3x faster than Dask&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large datasets (&amp;gt; 10M rows)&lt;/strong&gt;: Polars 8-15x faster than Pandas, 4x faster than Dask&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory usage&lt;/strong&gt;: Polars used 40-60% less memory than Pandas across all dataset sizes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The combination of performance, memory efficiency, and modern architecture made Polars the clear choice.&lt;/p&gt;
&lt;h2 id=&#34;polars-the-technical-breakthrough&#34;&gt;Polars: The Technical Breakthrough&lt;/h2&gt;
&lt;p&gt;&lt;img
  src=&#34;https://megaz.github.io/images/polar.png&#34;
  alt=&#34;Polars Logo&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;


*Polars: The high-performance data processing library that changed everything.  Credit to Ritchie Vink for this brilliant piece of &lt;a href=&#34;https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/*&#34;&gt;engineering&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Polars achieves its performance through several key innovations that address Pandas&amp;rsquo; fundamental limitations:&lt;/p&gt;
&lt;h3 id=&#34;lazy-evaluation-query-optimization-before-execution&#34;&gt;Lazy Evaluation: Query Optimization Before Execution&lt;/h3&gt;
&lt;p&gt;Pandas is eager as it executes operations immediately. Polars builds an execution plan and optimizes it before running anything:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Pandas: &amp;#34;Execute everything immediately!&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;massive_un_dataset.csv&amp;#39;&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Loads entire file into memory&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;geography&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;agg({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;population&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;})  &lt;span style=&#34;color:#75715e&#34;&gt;# Processes everything&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Polars: &amp;#34;Let me think about this first...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scan_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;massive_un_dataset.csv&amp;#39;&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Just creates a plan&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;group_by(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;geography&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;agg(pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;population&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()  &lt;span style=&#34;color:#75715e&#34;&gt;# Optimizes then executes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Polars can push down filters, eliminate unnecessary columns, and optimize joins before any actual processing happens.&lt;/p&gt;
&lt;h3 id=&#34;apache-arrow-memory-efficiency-that-actually-works&#34;&gt;Apache Arrow: Memory Efficiency That Actually Works&lt;/h3&gt;
&lt;p&gt;Polars uses Apache Arrow under the hood, which means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Zero-copy operations&lt;/strong&gt;: No unnecessary data copying&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Columnar storage&lt;/strong&gt;: Data is stored in columns, not rows (much more cache-friendly)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automatic memory management&lt;/strong&gt;: No more manual garbage collection tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The memory usage graph went from a jagged mountain range to a flat line.&lt;/p&gt;
&lt;h2 id=&#34;the-migration-key-performance-optimizations&#34;&gt;The Migration: Key Performance Optimizations&lt;/h2&gt;
&lt;p&gt;The migration wasn&amp;rsquo;t just about switching libraries; it was about fundamentally rethinking how we process data. Here are the key optimizations that delivered the biggest impact:&lt;/p&gt;
&lt;h3 id=&#34;1-eliminate-iterrows---no-exceptions&#34;&gt;1. Eliminate &lt;code&gt;iterrows()&lt;/code&gt; - No Exceptions&lt;/h3&gt;
&lt;p&gt;The first rule of Polars optimization: if you&amp;rsquo;re using &lt;code&gt;iterrows()&lt;/code&gt;, you&amp;rsquo;re doing it wrong. We replaced every single &lt;code&gt;iterrows()&lt;/code&gt; call with vectorized operations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before (Pandas):&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add_dimension_and_attribute_lists&lt;/span&gt;(df):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; index, row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iterrows():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        dimension_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dimension_columns:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;notna(row[col]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                dimension_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(row[col])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;at[index, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dimensions&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(dimension_list)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; df
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;After (Polars):&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add_dimension_and_attribute_lists&lt;/span&gt;(df):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    df_pl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pandas(df)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    df_pl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df_pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;with_columns([
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat_str(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(col)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(col)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_not_null()),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            separator&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;alias(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dimensions&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dimension_columns
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; df_pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_pandas()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Result: 29x faster execution time.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-memory-optimization-through-data-types&#34;&gt;2. Memory Optimization Through Data Types&lt;/h3&gt;
&lt;p&gt;We optimized our data types and saw dramatic memory reductions:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Optimized data types for memory efficiency&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;polars_kwargs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dtypes&amp;#39;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;concept_id&amp;#39;&lt;/span&gt;: pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Categorical,  &lt;span style=&#34;color:#75715e&#34;&gt;# Categorical for repeated strings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;concept_name&amp;#39;&lt;/span&gt;: pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Categorical,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;concept_role&amp;#39;&lt;/span&gt;: pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Categorical,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;codeList&amp;#39;&lt;/span&gt;: pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Categorical,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;infer_schema_length&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;# Limit schema inference&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;low_memory&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;# Enable low memory mode&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Use Int32 instead of Int64, Float32 instead of Float64&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;polars_dtypes[col] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Int32  &lt;span style=&#34;color:#75715e&#34;&gt;# 50% memory reduction&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;polars_dtypes[col] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Float32  &lt;span style=&#34;color:#75715e&#34;&gt;# 50% memory reduction&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Result: 50% reduction in memory usage.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-migration-real-world-performance-results&#34;&gt;The Migration: Real-World Performance Results&lt;/h2&gt;
&lt;p&gt;After three months of running both systems in parallel, here are the hard numbers:&lt;/p&gt;
&lt;h3 id=&#34;processing-speed-the-good-stuff&#34;&gt;Processing Speed (The Good Stuff)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Small datasets (&amp;lt; 1M rows)&lt;/strong&gt;: 2-3x faster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medium datasets (1-10M rows)&lt;/strong&gt;: 4-6x faster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large datasets (&amp;gt; 10M rows)&lt;/strong&gt;: 8-12x faster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Our monster dataset (50M+ rows)&lt;/strong&gt;: 15x faster&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;memory-efficiency-the-real-game-changer&#34;&gt;Memory Efficiency (The Real Game-Changer)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Peak memory usage&lt;/strong&gt;: 60% reduction on average&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory consistency&lt;/strong&gt;: No more out-of-memory errors (seriously, zero)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Garbage collection&lt;/strong&gt;: Went from &amp;ldquo;constant headache&amp;rdquo; to &amp;ldquo;barely noticeable&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-real-test-our-most-complex-pipeline&#34;&gt;The Real Test: Our Most Complex Pipeline&lt;/h3&gt;
&lt;p&gt;Our most complex pipeline involved running the ETL for ILO with a large amount of data performing complex transformations. This pipeline typically took 45 minutes to run and occasionally crashed with out-of-memory errors.&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;https://megaz.github.io/images/ilo_time_bar_pretty_fixed.png&#34;
  alt=&#34;ILO Processing Time&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;


&lt;em&gt;ILO dataset processing time: Pandas vs Polars performance comparison&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;https://megaz.github.io/images/ilo_memory_bar_pretty_fixed.png&#34;
  alt=&#34;ILO Memory Usage&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;


&lt;em&gt;ILO dataset memory consumption: Dramatic reduction with Polars&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pandas approach:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load everything into memory immediately&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pop_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(data_paths[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gdp_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(data_paths[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Multiple joins (each creating a copy)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pop_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;merge(gdp_data, on&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;country_code&amp;#39;&lt;/span&gt;, how&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Complex calculations and aggregations...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Polars approach:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Read datasets lazily (no memory usage yet!)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;datasets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scan_csv(path) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; path &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data_paths]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Chain everything together with lazy evaluation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (datasets[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(datasets[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], on&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;country_code&amp;#39;&lt;/span&gt;, how&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;with_columns([&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect())  &lt;span style=&#34;color:#75715e&#34;&gt;# Only now does it execute&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pandas: 45 minutes, 8GB RAM, occasional crashes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Polars: 6 minutes, 2GB RAM, zero crashes&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That&amp;rsquo;s a 7.5x speedup and 75% memory reduction on our most complex pipeline.&lt;/p&gt;
&lt;h2 id=&#34;the-real-world-impact&#34;&gt;The Real-World Impact&lt;/h2&gt;
&lt;p&gt;The performance improvements transformed our team&amp;rsquo;s productivity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster experimentation&lt;/strong&gt;: Analysis requests went from &amp;ldquo;come back in 2 hours&amp;rdquo; to &amp;ldquo;give me 5 minutes&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Larger datasets&lt;/strong&gt;: We can now process datasets that were previously impossible&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost reduction&lt;/strong&gt;: 40% reduction in cloud compute costs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster delivery&lt;/strong&gt;: Policy reports that used to take 3 days now take 6 hours&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; A recent policy analysis request for education spending and GDP growth relationships across all countries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Previous workflow&lt;/strong&gt;: 2-3 hours initial processing, 1-2 hours per iteration, 2-3 business days total&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Current workflow&lt;/strong&gt;: 15-20 minutes initial processing, 5-10 minutes per iteration, same day delivery&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;code-migration-patterns&#34;&gt;Code Migration Patterns&lt;/h2&gt;
&lt;p&gt;Here are the most common patterns we encountered during our migration, with before/after comparisons to illustrate the transformation:&lt;/p&gt;
&lt;h3 id=&#34;reading-data&#34;&gt;Reading Data&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Pandas: Load everything immediately&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data.csv&amp;#39;&lt;/span&gt;, parse_dates&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Polars: Lazy loading with smart parsing&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scan_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data.csv&amp;#39;&lt;/span&gt;, try_parse_dates&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;grouping-and-aggregation&#34;&gt;Grouping and Aggregation&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Pandas: Dictionary-based aggregation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;category&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;agg({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;count&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset_index()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Polars: Functional aggregation (much cleaner!)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;group_by(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;category&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;agg([
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            ]))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;complex-filtering&#34;&gt;Complex Filtering&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Pandas: Boolean indexing (gets messy fast)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;filtered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2020&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              (df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;country&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isin(countries)) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              (df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; threshold)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Polars: Clean, readable filtering&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;filtered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2020&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;country&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_in(countries)) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; threshold)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;joins&#34;&gt;Joins&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Pandas: Multiple merge operations&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;merge(df2, on&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;, how&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;merge(df3, on&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;, how&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Polars: Chain joins efficiently&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (df1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(df2, on&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;, how&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(df3, on&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;, how&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;key-lessons-learned&#34;&gt;Key Lessons Learned&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start Small, Scale Gradually&lt;/strong&gt;: Begin with your most performance-critical pipelines&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leverage Lazy Evaluation&lt;/strong&gt;: Chain operations for optimal performance rather than multiple collect() calls&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimize Data Types&lt;/strong&gt;: Use appropriate data types to reduce memory usage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Invest in Team Training&lt;/strong&gt;: The functional programming paradigm requires a mindset shift&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion-the-strategic-impact&#34;&gt;Conclusion: The Strategic Impact&lt;/h2&gt;
&lt;p&gt;The migration from Pandas to Polars delivered substantial value beyond performance improvements and it fundamentally transformed our data processing capabilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;8-12x performance improvements&lt;/strong&gt; for large datasets&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;60% reduction in memory usage&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feedback cycles&lt;/strong&gt; went from hours to minutes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;40% cost savings&lt;/strong&gt; on cloud infrastructure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Team productivity&lt;/strong&gt; increased dramatically&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The technical migration represents only the beginning. The real value emerges from how these tools change your team&amp;rsquo;s approach to data processing, enabling more sophisticated analysis and faster iteration cycles.&lt;/p&gt;
&lt;p&gt;For organizations considering similar migrations, I recommend starting with a comprehensive evaluation of your current bottlenecks and conducting proof-of-concept testing with your most challenging datasets. The performance gains and operational improvements we achieved demonstrate the significant value of modern data processing frameworks.&lt;/p&gt;
&lt;p&gt;The future looks bright: we&amp;rsquo;re now exploring parallel processing with Polars&amp;rsquo; lazy evaluation, streaming data processing for real-time analytics, and GPU acceleration for even faster processing. The migration to Polars was just the beginning of our data engineering transformation.&lt;/p&gt;
</description>
      <author>Zahir Abdi</author>
      <guid>https://megaz.github.io/posts/pandas-to-polars-migration/</guid>
      <pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate>
    </item>
    
  </channel>
</rss>
